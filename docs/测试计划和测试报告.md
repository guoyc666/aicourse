# 测试计划

文件状态：

- [ ] 草稿
- [x] 正式发布
- [ ] 正在修改

|            |                         |
| ---------- | ----------------------- |
| 文件标识： | Company-Project-IT-PLAN |
| 当前版本： | 1.0                     |
| 作 者：    | 郭彦晨                  |
| 完成日期： | 2025-09-25              |

[TOC]

---

## 1. 测试范围与主要内容

测试范围：本次系统测试覆盖 AI 辅助学习系统的核心功能模块与关键非功能需求，包含但不限于：

（1）**功能测试**

- 教师端：资源上传、题库管理、知识图谱查看
- 学生端：AI 问答、学习路径查看、题目练习
- 管理端：用户管理、系统监控
- 模型端：RAG 检索、知识图谱节点关联、问答质量验证

（2）**健壮性测试**

- 资源格式异常处理（损坏 PDF、空白资源）
- 提问输入异常、多轮对话异常情况
- 网络延迟、资源加载失败等场景

（3）**性能测试**

- 向量检索延迟
- 图谱加载速度
- QPS 基准压力测试（简化）

（4）**用户界面测试**

- 页面布局适配性（桌面端为主）
- 表单输入与错误提示
- 界面跳转流程一致性

（5）**安全性测试**

- 登录认证与权限控制
- API 安全（避免越权访问）
- 文件上传安全性

（6）**安装与反安装测试**

- 环境初始化脚本正确性
- 数据库首次创建与清理
- 模型文件加载与更新

## 2. 测试方法

- 黑盒测试：基于需求与用例设计覆盖功能路径，验证输入-输出行为。
- 白盒测试：针对关键后端逻辑（解析、向量化、知识图谱写入）设计单元测试，覆盖边界条件。
- 自动化测试：对核心 API 编写 pytest 自动化用例并集成到 CI；前端使用基本脚本验证页面流程。
- 性能测试：使用 JMeter 或 k6 对检索与问答接口进行负载测试（小规模并发模拟）。
- 手工验收测试：按演示用例由产品/教师进行最终验收。

## 3. 测试环境与测试辅助工具

测试环境：

| 项目         | 配置                      |
| ------------ | ------------------------- |
| 操作系统     | Windows 10 / Ubuntu 20.04 |
| 浏览器       | Chrome / Edge             |
| 后端运行环境 | Python 3.10 + FastAPI     |
| 数据库       | MySQL 8.0、Neo4j Desktop  |
| AI 运行环境  | GPU（24GB 显存）          |
| 前端环境     | Node.js 16 + Vue3         |

测试辅助工具：

| 工具            | 用途         |
| --------------- | ------------ |
| Postman         | API 接口测试 |
| PyTest          | 单元测试执行 |
| JMeter          | 性能测试     |
| Chrome DevTools | 前端调试     |
| GitHub Issues   | 缺陷管理     |
| Neo4j Browser   | 图谱验证     |
| VSCode          | 调试代码     |

## 4. 测试完成准则

基于测试用例的准则：

1. 功能性测试用例通过率达到 100%。
2. 非功能性（性能、稳定性、安全）测试用例通过率达到 ≥95%。

补充缺陷密度规则（如需更严）： 3. 在任意连续 24 小时的主要测试周期内，严重/关键缺陷密度 ≤ 1（每千行代码或每若干用例衡量），且无阻断性缺陷未关闭。

其他验收条件：

- 演示场景（指定用例）能稳定运行并通过产品/教师验收。
- 部署脚本能够在演示环境成功执行，数据可初始化与回滚。

## 5. 人员与任务表

| 人员     | 角色              | 职责/任务                                          | 时间             |
| -------- | ----------------- | -------------------------------------------------- | ---------------- |
| 郭彦晨   | 测试负责人 / 开发 | 组织测试计划，接口与模型相关测试用例，修复一线支持 | 周期内持续       |
| 张诏     | 测试工程师        | 知识图谱、导入与可视化测试、数据准备               | 周 5-8 重点参与  |
| 冯轶开   | 前端测试          | 前端交互、上传流程与 RBAC 测试                     | 周 3-6 重点参与  |
| 龙功泽   | 部署/CI           | 部署、环境搭建、性能测试支持                       | 周 2-10 重点参与 |
| 全体成员 | 验收/回归         | 参与回归测试、验收演示                             | 周 10-12         |

测试任务里程碑：

- 用例设计完成：周 2
- 单元与接口自动化覆盖：周 5
- 集成与功能测试完成：周 8-10
- 性能测试与回归：周 10
- 验收测试与报告：周 11-12

## 6. 缺陷管理与改错计划

缺陷管理流程：

1. 提交：测试人员在 GitHub Issues 提交缺陷，填写重现步骤、环境、日志与优先级。
2. 分类与分级：项目负责人/开发进行初步分级（Blocker/Critical/Major/Minor/Trivial）。
3. 指派：缺陷指派到责任开发人员并设定修复 SLA（见下）。
4. 修复：开发修复并提交 PR，关联缺陷编号。
5. 验证：测试人员验证修复并在关闭前执行回归用例。
6. 关闭：验证通过后由测试负责人关闭缺陷单。

缺陷优先级与建议 SLA：

- Blocker（阻断）：24 小时内响应并优先处理，直至解决或临时规避。
- Critical（严重）：48 小时内修复或提供补救方案。
- Major（主要）：3 个工作日内修复。
- Minor/Trivial：在下一个迭代/发布前处理。

改错策略：

- 紧急补丁：对阻断性缺陷采用 hotfix 分支策略，单独回归验证后合并主分支并发布演示镜像。
- 回归测试：所有已修复缺陷须通过对应回归测试用例；关键路径在每次集成后执行烟雾测试。

报告与记录：

- 周度测试报告包含：测试用例执行情况、已发现缺陷统计（按严重度）、阻断/未解决缺陷列表、测试进度与风险。
- 最终测试报告包含：测试总览、未解决缺陷清单、性能与稳定性结论与建议。

# 测试报告

